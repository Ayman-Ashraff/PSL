{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHcIasYZX4Rt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from keras.callbacks import TensorBoard\n",
        "import keras\n",
        "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyEIyLKXNyip",
        "outputId": "21dbc2a4-6d28-4454-d03b-79d823904b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Kaggle users: this is where you import the keypoints files you've generated\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLgUepC7rCa2"
      },
      "outputs": [],
      "source": [
        "# code to read desired number of keypoint data instances based on classes or videos. 55 rows per video and 550 rows per class.\n",
        "\n",
        "start_limit = 0\n",
        "end_limit = 16500\n",
        "\n",
        "X=np.load('/content/drive/MyDrive/PslKeypoints/psl_keypoints.npy')\n",
        "# X = X[start_limit:end_limit]\n",
        "\n",
        "Y=np.load('/content/drive/MyDrive/PslKeypoints/psl_labels.npy')\n",
        "# Y = Y[start_limit:end_limit]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUmGPfMZNp8i"
      },
      "outputs": [],
      "source": [
        "# code to skip over a few augmentations (optional)\n",
        "\n",
        "# new_X = []\n",
        "# new_Y = []\n",
        "\n",
        "# i = 0\n",
        "# while i < X.shape[0]:\n",
        "#     for j in range(15):\n",
        "#       # if j!=2 and j!=3 and j!=6 and j!=7:\n",
        "#       #   new_X.append(X[i+j])\n",
        "#       #   new_Y.append(Y[i+j])\n",
        "#       new_X.append(X[i+j])\n",
        "#       new_Y.append(Y[i+j])\n",
        "#     i += 55\n",
        "\n",
        "# X = np.array(new_X)\n",
        "# Y = np.array(new_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELy_zKVD2mKi"
      },
      "outputs": [],
      "source": [
        "# splitting and preparing data for training\n",
        "\n",
        "y = to_categorical(Y).astype(int)\n",
        "# y = y[:,:]\n",
        "print(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "print(X.shape)\n",
        "print(y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8r8wIHaM3Ia"
      },
      "outputs": [],
      "source": [
        "del X\n",
        "del Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDUkKtY43_zT"
      },
      "outputs": [],
      "source": [
        "# # Uncomment whichever model you want to use\n",
        "\n",
        "\n",
        "# # LSTM model\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(45,258)))\n",
        "# model.add(Dropout(0.4))      ##0.4\n",
        "# model.add(LSTM(256, return_sequences=True, activation='relu'))\n",
        "# model.add(Dropout(0.4))      ##0.4\n",
        "# model.add(LSTM(256, return_sequences=False, activation='relu'))\n",
        "# model.add(Dropout(0.4))      ##//\n",
        "\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# model.add(Dense(256, activation='relu'))   ##// A2\n",
        "# model.add(Dense(128, activation='relu'))   ##//\n",
        "# model.add(Dense(64, activation='relu'))    ##//\n",
        "\n",
        "# model.add(Dense(15, activation='softmax'))\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # GRU model\n",
        "# from tensorflow.keras.layers import GRU\n",
        "# model = Sequential()\n",
        "# model.add(GRU(128, return_sequences=True, activation='relu', input_shape=(45, 258)))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(GRU(256, return_sequences=True, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(GRU(256, return_sequences=False, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# model.add(Dense(15, activation='softmax'))\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Simple RNN model\n",
        "# from tensorflow.keras.layers import SimpleRNN\n",
        "# model = Sequential()\n",
        "# model.add(SimpleRNN(128, return_sequences=True, activation='relu', input_shape=(45, 258)))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(SimpleRNN(256, return_sequences=True, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(SimpleRNN(256, return_sequences=False, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# model.add(Dense(15, activation='softmax'))\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Bidirectional RNN\n",
        "# from tensorflow.keras.layers import Bidirectional, SimpleRNN\n",
        "# model = Sequential()\n",
        "# model.add(Bidirectional(SimpleRNN(128, return_sequences=True, activation='relu'), input_shape=(45, 258)))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(Bidirectional(SimpleRNN(256, return_sequences=True, activation='relu')))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(Bidirectional(SimpleRNN(256, return_sequences=False, activation='relu')))\n",
        "# # model.add(Dropout(0.3))\n",
        "\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# # model.add(Dense(256, activation='relu'))\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# model.add(Dense(15, activation='softmax'))\n",
        "\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPWDKhZ5__16"
      },
      "outputs": [],
      "source": [
        "# setting hyperparameters and training\n",
        "\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train, y_train,validation_data=(X_test,y_test), epochs=600, callbacks=[callback],batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Increase the epoch interval\n",
        "step_size = 3\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.plot(range(1, len(model.history.history['categorical_accuracy']) + 1, step_size), model.history.history['categorical_accuracy'][::step_size], label=\"training accuracy\")\n",
        "plt.plot(range(1, len(model.history.history['val_categorical_accuracy']) + 1, step_size), model.history.history[\"val_categorical_accuracy\"][::step_size], label=\"validation accuracy\")\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.plot(range(1, len(model.history.history['loss']) + 1, step_size), model.history.history[\"loss\"][::step_size], label=\"training loss\")\n",
        "plt.plot(range(1, len(model.history.history['val_loss']) + 1, step_size), model.history.history[\"val_loss\"][::step_size], label=\"validation loss\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "of5RxjLQ9OcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwvrurR8S1wu"
      },
      "outputs": [],
      "source": [
        "model.save('RP_RNN_PSL_15C_45F_256B_20T_.h5')\n",
        "# del model, X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-090YgpWUO7"
      },
      "outputs": [],
      "source": [
        "# accuracy test\n",
        "\n",
        "\n",
        "yhat = model.predict(X_test)\n",
        "ytrue = np.argmax(y_test, axis=1).tolist()\n",
        "yhat = np.argmax(yhat, axis=1).tolist()\n",
        "\n",
        "cm = confusion_matrix(ytrue, yhat)\n",
        "# multilabel_confusion_matrix(ytrue, yhat)\n",
        "\n",
        "# Creating a heatmap of the confusion matrix\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6IlJi0sWkIT"
      },
      "outputs": [],
      "source": [
        "accuracy_score(ytrue, yhat)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}